//! # Embedding Model Download with Progress
//!
//! æä¾›å¸¦è¿›åº¦æ˜¾ç¤ºçš„æ¨¡å‹ä¸‹è½½åŠŸèƒ½ã€‚

use std::path::Path;
use std::time::Duration;

use tracing::{error, info};

use crate::error::{CisError, Result};
use crate::storage::unified_paths::UnifiedPaths;

/// æ¨¡å‹æ–‡ä»¶ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct ModelFile {
    pub name: &'static str,
    pub url: &'static str,
    pub size_bytes: u64,
    pub path: std::path::PathBuf,
}

/// Nomic Embed Text v1.5 æ¨¡å‹
#[allow(clippy::incompatible_msrv)]
pub const NOMIC_EMBED_MODEL: ModelFile = ModelFile {
    name: "nomic-embed-text-v1.5",
    url: "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5/resolve/main/onnx/model.onnx",
    size_bytes: 130_000_000, // ~130MB
    path: std::path::PathBuf::new(), // åœ¨è¿è¡Œæ—¶è®¾ç½®
};

/// Tokenizer æ–‡ä»¶
#[allow(clippy::incompatible_msrv)]
pub const NOMIC_TOKENIZER: ModelFile = ModelFile {
    name: "tokenizer",
    url: "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5/resolve/main/tokenizer.json",
    size_bytes: 500_000, // ~500KB
    path: std::path::PathBuf::new(),
};

/// è·å–æ¨¡å‹æ–‡ä»¶è·¯å¾„
pub fn get_model_paths() -> (ModelFile, ModelFile) {
    let model_dir = UnifiedPaths::models_dir().join("nomic-embed-text-v1.5");
    
    let mut model = NOMIC_EMBED_MODEL.clone();
    model.path = model_dir.join("model.onnx");
    
    let mut tokenizer = NOMIC_TOKENIZER.clone();
    tokenizer.path = model_dir.join("tokenizer.json");
    
    (model, tokenizer)
}

/// æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
pub fn is_model_downloaded() -> bool {
    let (model, tokenizer) = get_model_paths();
    model.path.exists() && tokenizer.path.exists()
}

/// åŒæ­¥ä¸‹è½½æ–‡ä»¶ï¼ˆå¸¦ç®€å•è¿›åº¦ï¼‰
pub fn download_file_sync(url: &str, dest: &Path, description: &str) -> Result<()> {
    info!("Downloading {} from {}", description, url);
    
    // åˆ›å»ºçˆ¶ç›®å½•
    if let Some(parent) = dest.parent() {
        std::fs::create_dir_all(parent)
            .map_err(|e| CisError::io(format!("Failed to create directory: {}", e)))?;
    }
    
    // åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    let temp_path = dest.with_extension("tmp");
    
    // å‘é€è¯·æ±‚
    let response = reqwest::blocking::get(url)
        .map_err(|e| CisError::network(format!("Failed to download: {}", e)))?;
    
    if !response.status().is_success() {
        return Err(CisError::network(format!(
            "Download failed with status: {}",
            response.status()
        )));
    }
    
    // è·å–å†…å®¹
    let content = response.bytes()
        .map_err(|e| CisError::network(format!("Failed to read response: {}", e)))?;
    
    // å†™å…¥ä¸´æ—¶æ–‡ä»¶
    std::fs::write(&temp_path, &content)
        .map_err(|e| CisError::io(format!("Failed to write file: {}", e)))?;
    
    // é‡å‘½åä¸ºæœ€ç»ˆæ–‡ä»¶
    std::fs::rename(&temp_path, dest)
        .map_err(|e| CisError::io(format!("Failed to rename file: {}", e)))?;
    
    info!("Successfully downloaded {} to {}", description, dest.display());
    
    Ok(())
}

/// ä¸‹è½½æ¨¡å‹ï¼ˆå¸¦é‡è¯•ï¼‰
pub async fn download_model_with_retry(max_retries: u32) -> Result<()> {
    let (model, tokenizer) = get_model_paths();
    
    // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if is_model_downloaded() {
        println!("âœ“ å‘é‡æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½");
        return Ok(());
    }
    
    println!("ğŸ“¥ å‡†å¤‡ä¸‹è½½å‘é‡æ¨¡å‹ (Nomic Embed Text v1.5)");
    println!("   æ¨¡å‹å¤§å°: ~130 MB");
    println!("   Tokenizer: ~500 KB");
    println!("   ä¿å­˜ä½ç½®: {}", model.path.parent().unwrap().display());
    println!();
    
    // ä¸‹è½½æ¨¡å‹æ–‡ä»¶
    let mut last_error = None;
    for attempt in 1..=max_retries {
        if attempt > 1 {
            println!("\nâ³ é‡è¯•ä¸‹è½½ (å°è¯• {}/{})...", attempt, max_retries);
            tokio::time::sleep(Duration::from_secs(2)).await;
        }
        
        println!("ğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶...");
        
        // ä½¿ç”¨ blocking åœ¨ spawn_blocking ä¸­æ‰§è¡Œ
        let url = model.url.to_string();
        let path = model.path.clone();
        
        match tokio::task::spawn_blocking(move || {
            download_file_sync(&url, &path, "model.onnx")
        }).await {
            Ok(Ok(_)) => {
                println!("âœ“ æ¨¡å‹æ–‡ä»¶ä¸‹è½½å®Œæˆ");
                break;
            }
            Ok(Err(e)) => {
                eprintln!("âœ— ä¸‹è½½å¤±è´¥: {}", e);
                last_error = Some(e);
                
                // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                let temp = model.path.with_extension("tmp");
                let _ = std::fs::remove_file(&temp);
            }
            Err(e) => {
                eprintln!("âœ— ä»»åŠ¡å¤±è´¥: {}", e);
                last_error = Some(CisError::other(format!("Task failed: {}", e)));
            }
        }
    }
    
    if let Some(e) = last_error {
        return Err(e);
    }
    
    // ä¸‹è½½ tokenizer
    println!();
    println!("ğŸ“¥ æ­£åœ¨ä¸‹è½½ tokenizer...");
    
    let url = tokenizer.url.to_string();
    let path = tokenizer.path.clone();
    
    match tokio::task::spawn_blocking(move || {
        download_file_sync(&url, &path, "tokenizer.json")
    }).await {
        Ok(Ok(_)) => {
            println!("âœ“ Tokenizer ä¸‹è½½å®Œæˆ");
        }
        Ok(Err(e)) => {
            eprintln!("[WARNING] Tokenizer ä¸‹è½½å¤±è´¥: {}", e);
            eprintln!("   æ¨¡å‹å¯èƒ½ä»å¯ç”¨ï¼Œä½†å»ºè®®é‡æ–°ä¸‹è½½ã€‚");
        }
        Err(e) => {
            eprintln!("[WARNING] Tokenizer ä¸‹è½½å¤±è´¥: {}", e);
        }
    }
    
    println!();
    println!("[OK] å‘é‡æ¨¡å‹ä¸‹è½½å®Œæˆï¼");
    println!("   æ¨¡å‹è·¯å¾„: {}", model.path.display());
    
    Ok(())
}

/// è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆéäº¤äº’å¼ï¼‰
pub async fn auto_download_model() -> Result<bool> {
    if is_model_downloaded() {
        return Ok(true);
    }
    
    match download_model_with_retry(3).await {
        Ok(_) => Ok(true),
        Err(e) => {
            error!("Failed to auto-download model: {}", e);
            Ok(false)
        }
    }
}

/// éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
pub fn verify_model() -> Result<bool> {
    let (model, tokenizer) = get_model_paths();
    
    if !model.path.exists() || !tokenizer.path.exists() {
        return Ok(false);
    }
    
    // æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆæ¨¡å‹åº”è¯¥ >100MBï¼‰
    let model_meta = std::fs::metadata(&model.path)
        .map_err(|e| CisError::io(format!("Failed to read model metadata: {}", e)))?;
    
    if model_meta.len() < 100_000_000 {
        return Ok(false); // æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½ä¸‹è½½ä¸å®Œæ•´
    }
    
    Ok(true)
}

/// åˆ é™¤å¹¶é‡æ–°ä¸‹è½½æ¨¡å‹
pub async fn redownload_model() -> Result<()> {
    let (model, tokenizer) = get_model_paths();
    
    // åˆ é™¤æ—§æ–‡ä»¶
    if model.path.exists() {
        tokio::fs::remove_file(&model.path).await
            .map_err(|e| CisError::io(format!("Failed to remove old model: {}", e)))?;
    }
    
    if tokenizer.path.exists() {
        tokio::fs::remove_file(&tokenizer.path).await
            .map_err(|e| CisError::io(format!("Failed to remove old tokenizer: {}", e)))?;
    }
    
    // é‡æ–°ä¸‹è½½
    download_model_with_retry(3).await
}

/// è·å–æ¨¡å‹ä¸‹è½½çŠ¶æ€
pub fn get_download_status() -> DownloadStatus {
    let (model, tokenizer) = get_model_paths();
    
    DownloadStatus {
        model_exists: model.path.exists(),
        tokenizer_exists: tokenizer.path.exists(),
        model_path: model.path,
        tokenizer_path: tokenizer.path,
        is_complete: is_model_downloaded(),
    }
}

/// ä¸‹è½½çŠ¶æ€
#[derive(Debug, Clone)]
pub struct DownloadStatus {
    pub model_exists: bool,
    pub tokenizer_exists: bool,
    pub model_path: std::path::PathBuf,
    pub tokenizer_path: std::path::PathBuf,
    pub is_complete: bool,
}

impl DownloadStatus {
    pub fn print(&self) {
        println!("å‘é‡æ¨¡å‹çŠ¶æ€:");
        println!("  æ¨¡å‹æ–‡ä»¶: {}", if self.model_exists { "âœ“ å·²ä¸‹è½½" } else { "âœ— æœªä¸‹è½½" });
        println!("  Tokenizer: {}", if self.tokenizer_exists { "âœ“ å·²ä¸‹è½½" } else { "âœ— æœªä¸‹è½½" });
        if self.model_exists {
            println!("  è·¯å¾„: {}", self.model_path.display());
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_model_paths() {
        let (model, tokenizer) = get_model_paths();
        assert!(model.path.to_string_lossy().contains("nomic-embed"));
        assert!(tokenizer.path.to_string_lossy().contains("tokenizer"));
    }

    #[test]
    fn test_download_status() {
        let status = get_download_status();
        // åªæ˜¯æµ‹è¯•ä¸ panic
        println!("{:?}", status);
    }
}
